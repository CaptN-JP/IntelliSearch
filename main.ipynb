{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2  # For PDF reading\n",
    "import sentencepiece  # Ensure sentencepiece is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/captn_jp/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4370969c575f4038a7ccaee22930c563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Semantic segmentation model (BigBird)\n",
    "tokenizer_segmentation = AutoTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model_segmentation = AutoModel.from_pretrained('google/bigbird-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2ca4b430974db2ab7c7c3e0ff88e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bacbf198c34f4e89ac4df4a7dfcc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10c2026087b42fd80fcfc86e2f44e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f63022571040d6888c258a73790e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795c4b5c7ac84a79a6b73481c597ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bf2f9c04844c78a37e1accc0ac0e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3626226a444484b903f1ca593bb664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa09eab342a4822b3a96b642e60f98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e069611ab345efb5788b591058af97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac12daa53e444858c7c687b9f6a4c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aae998f18af4be3a2c175815018677a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d6da57b29f47d189a2cee330392457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec983217da45349a9588a680365898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5eef31718e74dbcb84bf26e645bf61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd652743df84eca8cf69d5b5f6c95f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31dfa50dc644e56b701595df11b8c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e6e873a0d42f1b5d3a60e0bf6aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model for generating answers\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model_llm = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read document\n",
    "def read_document(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Unsupported file type.\")\n",
    "        return None\n",
    "\n",
    "# Semantic Segmentation (Chunking)\n",
    "def semantic_segmentation(text, max_length=512):\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    for sentence in sentences:\n",
    "        if len(tokenizer_segmentation.tokenize(current_chunk + sentence)) < max_length:\n",
    "            current_chunk += sentence + '. '\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + '. '\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Embedding Chunks\n",
    "def embed_chunks(chunks):\n",
    "    embeddings = embedding_model.encode(chunks, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "# Build FAISS Index\n",
    "def build_faiss_index(embeddings):\n",
    "    embeddings_np = embeddings.cpu().detach().numpy().astype('float32')\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "    return index\n",
    "\n",
    "# Retrieve Relevant Chunks\n",
    "def retrieve_relevant_chunks(question, index, chunks, top_k=5):\n",
    "    question_embedding = embedding_model.encode([question], convert_to_tensor=True)\n",
    "    question_embedding_np = question_embedding.cpu().detach().numpy().astype('float32')\n",
    "    distances, indices = index.search(question_embedding_np, top_k)\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "    return relevant_chunks\n",
    "\n",
    "# Generate Answer Using LLM\n",
    "def generate_answer(question, context):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant. Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    inputs = tokenizer_llm.encode(prompt, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    summary_ids = model_llm.generate(inputs, max_length=150, early_stopping=True)\n",
    "    answer = tokenizer_llm.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document...\n",
      "Document processed and indexed successfully.\n",
      "Generating answer...\n",
      "Answer:\n",
      "The user can perform 4 different types of analysis using the chat interface. Keeping in mind the scalability, response time, cost associated and ability to add more features in future. The different analysis the product must be able to perform are: Generate Plots for Key Variables • Conduct Regression Analysis • Identify Key Columns for Analysis (for problem specified by the user)\n"
     ]
    }
   ],
   "source": [
    "# Main Execution Flow\n",
    "# 1. Read Document\n",
    "file_path = '/Users/captn_jp/Downloads/project_charter.pdf'  # Replace with your document's path\n",
    "document_text = read_document(file_path)\n",
    "\n",
    "# 2. Process Document\n",
    "print(\"Processing document...\")\n",
    "chunks = semantic_segmentation(document_text)\n",
    "embeddings = embed_chunks(chunks)\n",
    "index = build_faiss_index(embeddings)\n",
    "print(\"Document processed and indexed successfully.\")\n",
    "\n",
    "# 3. Ask a Question\n",
    "question = input(\"Ask a question about the document: \")\n",
    "\n",
    "# 4. Generate Answer\n",
    "print(\"Generating answer...\")\n",
    "relevant_chunks = retrieve_relevant_chunks(question, index, chunks)\n",
    "context = ' '.join(relevant_chunks)\n",
    "answer = generate_answer(question, context)\n",
    "print(\"Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
